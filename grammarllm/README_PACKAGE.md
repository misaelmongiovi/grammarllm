# ‚≠êÔ∏è GrammarLLM ‚Äî Grammar Constrained Natural Language Generation

**GrammarLLM** is a powerful Python library for **grammar-constrained text generation**, built on top of pre-trained Transformer models.
It allows you to define and apply constraints via formal grammars, ideal for classification, vocabulary restriction, and structured generation.

---

## üöÄ Features

* ‚úÖ **Grammar-constrained generation** ‚Äî define your own production rules
* ü§ó **Compatible with Hugging Face Transformers**
* ‚ö°Ô∏è **Linear-time decoding via deterministic PDA** ‚Äî efficient grammar-constrained generation

---

## üì¶ Requirements

* Python ‚â• 3.10
* ü§ó Transformers ‚â• 4.30.0
* PyTorch **or** TensorFlow
* A pre-trained causal language model (e.g., GPT-2, LLaMA)

---

## ‚öôÔ∏è Installation
The package is currently available on Test PyPI. To install the library, use the following command:

```bash
pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple grammarllm
```

---
## üîç Use Cases

### 1. üîÆ Classification

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from grammarllm import (
    generate_grammar_parameters,
    generate_text,
    get_parsing_table_and_map_tt,
    setup_logging,
    create_prompt,
    chat_template,
)

setup_logging()

productions = { 'S*': ["<<positive >> A", "<<negative >> B", "<<neutral >> C"],
                'A': ["<<happy>>", "<<peaceful>>", "<<joyful>>"],
                'B': ['<<sad>>', '<<angry>>', '<<frustrated>>'],
                'C': ['<<calm>>', '<<indifferent>>', '<<unemotional>>']
                }

system_prompt = """You are a hierarchical classification assistant. Your task is to classify the user input 
                    into one of the following hierarchical categories as shown in the followig examples\n\n"""

examples = [
    {"role": "user", "content": "I just got a promotion!"},
    {"role": "assistant", "content": "positive joyful"},

    {"role": "user", "content": "Nothing ever goes my way."},
    {"role": "assistant", "content": "negative frustrated"},

    {"role": "user", "content": "The lake was still and quiet."},
    {"role": "assistant", "content": "neutral calm"},

    {"role": "user", "content": "I miss my family so much."},
    {"role": "assistant", "content": "negative sad"}
]

prompt=create_prompt(
    prompt_input="It's raining and I feel a bit down.",
    system_prompt=system_prompt,
    examples=examples
)


model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")

pars_table, map_terminal_tokens = get_parsing_table_and_map_tt(tokenizer, productions)

LogitProcessor, Streamer = generate_grammar_parameters(
    tokenizer, pars_table, map_terminal_tokens
)

output = generate_text(model, tokenizer, prompt, LogitProcessor, Streamer, chat_template)
print(output)  # ‚Üí "negative sad"
```

---

### 2. üß© Vocabulary Restriction

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from grammarllm import (
    generate_grammar_parameters,
    generate_text,
    get_parsing_table_and_map_tt,
    setup_logging,
    create_prompt,
    chat_template,
)

setup_logging()

productions = {
'S*': [
    "<< Yes>> S*",
    "<< I'm>> S*",
    "<< very>> S*",
    "<< happy>> S*",
    "<< !>> S*",
    "<< so>> S*",
    "<< really>> S*",
    "<< excited>> S*",
    "<< today>> S*",
    "<< thanks>> S*",
    "<< you>> S*",
    "<< much>> S*",
    "<< great>> S*",
    "<< good>> S*",
    "<< fine>> S*",
    "<< amazing>> S*",
    ]
}

system_prompt = """You are a text generation assistant. When generating responses, you must use only the words that
                    appear in the provided examples below. You should not introduce any new words outside of those examples."""


examples = [
    {"role": "user", "content": "How are you?"},
    {"role": "assistant", "content": "I'm very happy"},

    {"role": "user", "content": "Is everything okay?"},
    {"role": "assistant", "content": "Yes, I'm so excited!"},

    {"role": "user", "content": "How was your day?"},
    {"role": "assistant", "content": "I'm really happy today!"},

    {"role": "user", "content": "Do you feel good?"},
    {"role": "assistant", "content": "Yes, I feel great, thanks!"},

    {"role": "user", "content": "What's up?"},
    {"role": "assistant", "content": "I'm fine, thank you so much!"},

    {"role": "user", "content": "Anything special today?"},
    {"role": "assistant", "content": "I'm very excited and happy today!"}
    ]

prompt=create_prompt(
    prompt_input="Say something of positive:",
    system_prompt=system_prompt,
    examples=examples
    )

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")

pars_table, map_terminal_tokens = get_parsing_table_and_map_tt(tokenizer, productions)

LogitProcessor, Streamer = generate_grammar_parameters(
    tokenizer, pars_table, map_terminal_tokens
)

output = generate_text(model, tokenizer, prompt, LogitProcessor, Streamer, chat_template)
print(output)  # ‚Üí "I'm happy"
```

---

### 3. üìê Structured Generation

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from grammarllm import (
    generate_grammar_parameters,
    generate_text,
    get_parsing_table_and_map_tt,
    setup_logging,
    create_prompt,
    chat_template,
)


setup_logging()
productions = {
    'S*': ["SUBJ PRED OBJ . S*"],
    'SUBJ': ["IRI", "BLANKNODE"],
    'PRED': ["IRI"],
    'OBJ': ["IRI", "BLANKNODE", "LITERAL"],
    'IRI': ["< URI >"],
    'BLANKNODE': ["<<_:>> NAME"],
    'LITERAL': ["\" STRING \" DESCRIPTION_LANG"],
    'DESCRIPTION_LANG': ["^^ IRI", "@ LANGTAG", "Œµ"],

    'URI': [
        # People
        "<<http://example.org/people/MarioRossi>>",
        "<<http://example.org/people/LuisaVerdi>>",
        "<<http://example.org/people/GiovanniBianchi>>",

        # Properties
        "<<http://example.org/properties/hasAge>>",
        "<<http://example.org/properties/hasProfession>>",
        "<<http://example.org/properties/hasSalary>>",

        # Other types or datatypes
        "<<http://www.w3.org/2001/XMLSchema#decimal>>",
        "<<http://www.w3.org/2001/XMLSchema#integer>>",
        "<<http://www.w3.org/2001/XMLSchema#string>>"
    ],

    'STRING':["alfanum STRING", "Œµ"],
    'NAME': ["ids NAME_C"],
    'NAME_C': ["idc NAME_C", "Œµ"],

    'LANGTAG': ['<<it >>', '<<en >>', '<<fr >>', '<<sp >>']
}

regex_alfanum = re.compile(r"[a-zA-Z0-9]+")  # es. "abc123"
regex_right_round_bracket = re.compile(r"\)$")  # match only ')'
regex_left_round_bracket = re.compile(r"\($")  # match only '('
regex_less_than = re.compile(r"^<$") # match only '<'
regex_greater_than = re.compile(r"^>$") # match only '>'
regex_double_quote = re.compile(r'^\"$') # match only '"'
regex_datatype = re.compile(r"^\^\^$")   # match only '^^'
regex_langtag = re.compile(r"^@$")       # match only '@'
regex_dot = re.compile(r"^\.$")  # match only '.'

# Starting identifier: must start with a letter or an underscore
regex_ids = re.compile(r'[A-Za-z_][A-Za-z0-9_-]*')
# Continuation identifier: cannot start with a letter or an underscore
regex_idc = re.compile(r'(?![A-Za-z_])[0-9_-][A-Za-z0-9_-]*')


regex_dict = {
    'regex_alfanum': regex_alfanum,
    'regex_)': regex_right_round_bracket,
    'regex_(': regex_left_round_bracket,
    'regex_<': regex_less_than,
    'regex_>': regex_greater_than,
    'regex_"': regex_double_quote,
    'regex_^^': regex_datatype,
    'regex_@': regex_langtag,
    'regex_.': regex_dot,

    'regex_ids':regex_ids,
    'regex_idc':regex_idc

    }


system_prompt = """You are an assistant that converts natural language sentences into RDF triples syntax.

Follow these rules:

1. Use URIs (`<...>`) for:
- Identifiable entities such as people, properties, or concepts.
- Example:
    <http://example.org/people/MarioRossi> <http://example.org/properties/hasFriend> <http://example.org/people/LuisaVerdi> .

2. Use literals (`"..."`) for:
- Plain values such as professions, cities, names, numbers, dates, or booleans.
- Add datatypes (`^^<...>`) or language tags (`@lang`) if needed.
- Examples:
    "engineer"@en  
    "40"^^<http://www.w3.org/2001/XMLSchema#integer>

3. Use blank nodes (`_:`) only if:
- The object is anonymous and has internal structure (i.e., it has its own properties).
- Example:
    <http://example.org/people/MarioRossi> <http://example.org/properties/hasAddress> _:b1 .
    _:b1 <http://example.org/properties/street> "Via Roma" .
    _:b1 <http://example.org/properties/city> "Milano" .

Never use a blank node (`_:`) for simple values like "engineer" or "teacher". Use a literal (`"..."`) instead.

Now use the following examples to generate clean and correct RDF triples from user input."""

examples = [
    {"role": "user", "content": "Mario Rossi is 40 years old."},
    {"role": "assistant", "content": "<http://example.org/people/MarioRossi> <http://example.org/properties/hasAge> \"40\" ^^<http://www.w3.org/2001/XMLSchema#integer> ."},

    {"role": "user", "content": "Luisa Verdi is an engineer."},
    {"role": "assistant", "content": "<http://example.org/people/LuisaVerdi> <http://example.org/properties/hasProfession> \"engineer\" @en ."},

    {"role": "user", "content": "Giovanni Bianchi earns 55000."},
    {"role": "assistant", "content": "<http://example.org/people/GiovanniBianchi> <http://example.org/properties/hasSalary> \"55000\" ^^<http://www.w3.org/2001/XMLSchema#decimal> ."},

    {"role": "user", "content": "Mario Rossi has an anonymous node as a contact."},
    {"role": "assistant", "content": "<http://example.org/people/MarioRossi> <http://example.org/properties/hasContact> _:ids ."},

    {"role": "user", "content": "Mario Rossi has the profession of teacher."},
    {"role": "assistant", "content": "<http://example.org/people/MarioRossi> <http://example.org/properties/hasProfession> \"teacher\" @en ."}
]

prompt=create_prompt(
    prompt_input="Giovanni Bianchi was born 30 years ago.",
    system_prompt=system_prompt,
    examples=examples
)

# Initialize tokenizer
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")

# Generate grammar parameters
pars_table, map_terminal_tokens = get_parsing_table_and_map_tt(
    tokenizer, 
    productions=productions, 
    regex_dict=regex_dict,
)

LogitProcessor, Streamer = generate_grammar_parameters(tokenizer, pars_table, map_terminal_tokens)
output = generate_text(model, tokenizer, prompt, LogitProcessor, Streamer, chat_template)
print(output) # Example output: "<http://example.org/people/GiovanniBianchi><http://example.org/properties/hasAge>"30"^^<http://www.w3.org/2001/XMLSchema#integer>."
  
```

---

## üõ† LL(prefix) Grammar Setup

GrammarLLm enforces syntactic correctness in linear time while maintaining expressiveness in grammar rule
definition. To achieve this, we propose LL(prefix) a novel formalization that generalizes the LL(1) class of [CFG](https://en.wikipedia.org/wiki/Context-free_grammar) enabling the user to define grammars without delving into the details of LLM subword tokenization.

### ‚úçÔ∏è Notation

* Use `<<some string>>` to **generate exact strings** the system handles tokenization
* Symbols **without** `<<>>` are **terminals**, which will be mapped into a set of tokens via regex
* **Uppercase** symbols (e.g., `A`,`B`,`C`) are **non-terminals**
* **Uppercase** `S*` symbol is the start symbol.
* Use `'Œµ'` for epsilon (empty) transitions


---

### üîç Regex Dictionary

You can use or extend the built-in `regex_dict` to define terminal patterns:

```python
import re

regex_alfanum = re.compile(r"[a-zA-Z0-9]+")        # e.g., "abc123"
regex_letters = re.compile(r"[a-zA-Z]+")           # e.g., "Hello"
regex_number = re.compile(r"\d+")                  # e.g., "12345"
regex_decimal = re.compile(r"\d+([.,]\d+)?")       # e.g., "3.14"
regex_var = re.compile(r"[a-zA-Z_][a-zA-Z0-9_]*")  # e.g., "_varName"
regex_left_round_bracket = re.compile(r"\(")       # e.g., "("
regex_right_round_bracket = re.compile(r"\)")      # e.g., ")"

regex_dict = {
    'regex_alfanum': regex_alfanum,
    'regex_letters': regex_letters,
    'regex_number': regex_number,
    'regex_decimal': regex_decimal,
    'regex_var': regex_var,
    'regex_(': regex_left_round_bracket,
    'regex_)': regex_right_round_bracket,
}
```

To define additional terminal patterns, simply extend `regex_dict`:

```python
new_regex = re.compile(r"your-pattern-here")
regex_dict['regex_custom'] = new_regex
```

Use custom regex keys as terminal symbols in your grammar productions.

---

## ‚ö†Ô∏è Limitations

* ‚ùå Beam search is **not supported**

---

## ü§ù Contributing

All contributions are welcome: bug reports, feature requests, improvements, or documentation updates.

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üì´ Contact

üìß Email:  
[gabriele.tuccio@phd.unict.it](mailto:gabriele.tuccio@phd.unict.it)  
[misael.mongiovi@unict.it](mailto:misael.mongiovi@unict.it)


